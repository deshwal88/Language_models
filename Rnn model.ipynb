{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import time\n",
    "import read_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.hidden_size_l1 = hidden_size_l1\n",
    "        self.hidden_size_l2 = hidden_size_l2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_vector_size = embedding_vector_size\n",
    "        self.lr = 0.0\n",
    "        self.max_grad_norm=max_grad_norm\n",
    "        \n",
    "        self.model=keras.Sequential()\n",
    "        with tf.device('/device:XLA_GPU:0'):\n",
    "            self.embedding_layer=keras.layers.Embedding(self.vocab_size,self.embedding_vector_size,\\\n",
    "                                           batch_input_shape=(self.batch_size,self.num_steps),trainable=True,name='embedding')\n",
    "            \n",
    "        lr_1=keras.layers.LSTMCell(self.hidden_size_l1)\n",
    "        lr_2=keras.layers.LSTMCell(self.hidden_size_l2)\n",
    "        stacked=keras.layers.StackedRNNCells([lr_1,lr_2])\n",
    "        self.rnn_layer=keras.layers.RNN(stacked,[self.batch_size,self.num_steps],return_state=False,trainable=True,\\\n",
    "                                        stateful=True)\n",
    "        \n",
    "        initial_state=tf.Variable((self.batch_size,embedding_vector_size),trainable=False)\n",
    "        self.rnn_layer.initial_state=initial_state\n",
    "        \n",
    "        self.dense_layer=keras.layers.Dense(self.vocab_size)\n",
    "        \n",
    "        self.activation=keras.layers.Activation('softmax')\n",
    "        self.optimizer=keras.optimizers.SGD(self.lr,clipnorm=self.max_grad_norm)\n",
    "        \n",
    "        self.model.add(self.embedding_layer)\n",
    "        self.model.add(self.rnn_layer)\n",
    "        self.model.add(self.dense_layer)\n",
    "        self.model.add(self.activation)\n",
    "        self.model.compile(self.optimizer,self.crossentropy)\n",
    "        self.model.summary()\n",
    "    \n",
    "    def crossentropy(self,ytrue,ypred):\n",
    "        return keras.losses.sparse_categorical_crossentropy(ytrue,ypred)\n",
    "    \n",
    "    def train_batch(self,input_,target_):\n",
    "        t_vars=self.model.trainable_variables\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            output=self.model(input_)\n",
    "            loss=self.crossentropy(target_,output)\n",
    "            cost=tf.reduce_sum(loss)/batch_size\n",
    "            \n",
    "        grads=tape.gradient(cost,t_vars)\n",
    "        clipped,value=tf.clip_by_global_norm(grads,self.max_grad_norm)\n",
    "        self.optimizer.apply_gradients(zip(clipped,t_vars))\n",
    "        return cost\n",
    "    \n",
    "    def test_batch(self,input_,target_):\n",
    "        output=self.model(input_)\n",
    "        loss=self.crossentropy(target_,output)\n",
    "        cost=tf.reduce_sum(loss)/batch_size\n",
    "        return cost\n",
    "        \n",
    "    @classmethod\n",
    "    def instance(cls):\n",
    "        return Model()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_one_epoch(m,data,is_training=True,verbose=True):\n",
    "    iters=0\n",
    "    cost=0\n",
    "    epoch_size=(len(data)//m.batch_size)//m.num_steps\n",
    "    start=time.time()\n",
    "    \n",
    "    for step,(x,y) in enumerate(read_func.data_to_batch(data,m.batch_size,m.num_steps)):\n",
    "        if is_training:\n",
    "            loss=m.train_batch(x,y)\n",
    "        else:\n",
    "            loss=m.test_batch(x,y)\n",
    "        \n",
    "        cost+=loss\n",
    "        iters+=m.num_steps\n",
    "    \n",
    "        if verbose and (step%(epoch_size//10))==10:\n",
    "            print(\"Iterations===> %d/%d    Perplexity===> %.2f    speed===> %.3f wps\"%(step,epoch_size,np.exp(cost/iters),\\\n",
    "                                                                                    (iters*m.batch_size)/(time.time()-start)))\n",
    "    \n",
    "    return np.exp(cost/iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=30\n",
    "num_steps=20\n",
    "hidden_size_l1=256\n",
    "hidden_size_l2=128\n",
    "vocab_size=10000\n",
    "embedding_vector_size=200\n",
    "max_grad_norm=5\n",
    "\n",
    "init_lr=1\n",
    "decay=0.5\n",
    "max_epoch_decay_lr = 4\n",
    "max_epoch=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (30, 20, 200)             2000000   \n",
      "_________________________________________________________________\n",
      "rnn (RNN)                    (30, 20, 128)             665090    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (30, 20, 10000)           1290000   \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (30, 20, 10000)           0         \n",
      "=================================================================\n",
      "Total params: 3,955,090\n",
      "Trainable params: 3,955,088\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "files=['ptb.train.txt','ptb.valid.txt','ptb.test.txt']\n",
    "vocab=read_func.build_vocab(files[0])\n",
    "train_data=read_func.reader(files[0],vocab)\n",
    "valid_data=read_func.reader(files[1],vocab)\n",
    "test_data=read_func.reader(files[2],vocab)\n",
    "m=Model.instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch===> 1    Learning_rate===>1.00\n",
      "Final_Perplexity===>359.99\n",
      "Valid_Perplexity===>213.90\n",
      "Epoch===> 2    Learning_rate===>1.00\n",
      "Final_Perplexity===>165.27\n",
      "Valid_Perplexity===>162.33\n",
      "Epoch===> 3    Learning_rate===>1.00\n",
      "Final_Perplexity===>125.70\n",
      "Valid_Perplexity===>145.74\n",
      "Epoch===> 4    Learning_rate===>1.00\n",
      "Final_Perplexity===>106.35\n",
      "Valid_Perplexity===>139.25\n",
      "Epoch===> 5    Learning_rate===>1.00\n",
      "Final_Perplexity===>94.20\n",
      "Valid_Perplexity===>136.20\n",
      "Epoch===> 6    Learning_rate===>0.50\n",
      "Final_Perplexity===>75.62\n",
      "Valid_Perplexity===>126.01\n",
      "Epoch===> 7    Learning_rate===>0.25\n",
      "Final_Perplexity===>64.21\n",
      "Valid_Perplexity===>123.28\n",
      "Epoch===> 8    Learning_rate===>0.12\n",
      "Final_Perplexity===>58.12\n",
      "Valid_Perplexity===>122.64\n",
      "Epoch===> 9    Learning_rate===>0.06\n",
      "Final_Perplexity===>54.99\n",
      "Valid_Perplexity===>122.57\n",
      "Epoch===> 10    Learning_rate===>0.03\n",
      "Final_Perplexity===>53.33\n",
      "Valid_Perplexity===>122.57\n",
      "Epoch===> 11    Learning_rate===>0.02\n",
      "Final_Perplexity===>52.43\n",
      "Valid_Perplexity===>122.49\n",
      "Epoch===> 12    Learning_rate===>0.01\n",
      "Final_Perplexity===>51.94\n",
      "Valid_Perplexity===>122.31\n",
      "Epoch===> 13    Learning_rate===>0.00\n",
      "Final_Perplexity===>51.68\n",
      "Valid_Perplexity===>122.11\n",
      "Epoch===> 14    Learning_rate===>0.00\n",
      "Final_Perplexity===>51.53\n",
      "Valid_Perplexity===>121.96\n",
      "Epoch===> 15    Learning_rate===>0.00\n",
      "Final_Perplexity===>51.46\n",
      "Valid_Perplexity===>121.87\n"
     ]
    }
   ],
   "source": [
    "K=tf.keras.backend\n",
    "for i in range(0,max_epoch):\n",
    "    \n",
    "    decay_rate=decay**max(i-max_epoch_decay_lr,0)\n",
    "    final_lr=init_lr*decay_rate\n",
    "    K.set_value(m.model.optimizer.learning_rate,final_lr)\n",
    "    print('Epoch===> %d    Learning_rate===>%.3f'%(i+1,m.model.optimizer.learning_rate))\n",
    "    \n",
    "    \n",
    "    prp=run_one_epoch(m,train_data,verbose=False)\n",
    "    print('Final_Perplexity===>%.2f'%(prp))\n",
    "    \n",
    "    \n",
    "    prp=run_one_epoch(m,valid_data,is_training=False,verbose=False)\n",
    "    print('Valid_Perplexity===>%.2f'%(prp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./assets\n"
     ]
    }
   ],
   "source": [
    "keras.models.save_model(m.model,'./rnn_model/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
